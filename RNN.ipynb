{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f8d0399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as F1\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import resnet18, resnet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d26ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customDataset(Dataset):\n",
    "    def __init__(self, videoList, classInd, subsample, size=(240, 320)):\n",
    "        with open(videoList) as f:\n",
    "            self.videoList = f.read().splitlines()\n",
    "        with open(classInd) as f:\n",
    "            classList = f.read().splitlines()\n",
    "            self.encodeClass = {x.split(\" \")[1] : int(x.split(\" \")[0]) - 1 for x in classList}\n",
    "            self.decodeClass = {int(x.split(\" \")[0]) -1 : x.split(\" \")[1] for x in classList}\n",
    "        self.subsample = subsample\n",
    "        self.n = len(classList)\n",
    "        self.size = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videoList)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        videoPath = self.videoList[idx].split(\" \")[0]\n",
    "        label = self.encodeClass[videoPath.split(\"/\")[0]]\n",
    "        video, _, _ = torchvision.io.read_video(\"./UCF-101/\"+videoPath, pts_unit='sec', output_format=\"TCHW\")\n",
    "        video = video[np.linspace(0, len(video)-1, self.subsample, dtype=\"int\")]\n",
    "        if video.shape[2:] != self.size:\n",
    "            video = F1.resize(video, size=self.size, antialias=False)\n",
    "#         video = F1.rgb_to_grayscale(video).transpose(0,1)\n",
    "        return video/255, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b499eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = customDataset(\"trainlist01.txt\", \"classInd.txt\", 8)\n",
    "testData = customDataset(\"testlist01.txt\", \"classInd.txt\", 8)\n",
    "\n",
    "trainDataloader = DataLoader(trainingData, batch_size=32, shuffle=True, pin_memory=True)\n",
    "validationDataloader = DataLoader(testData, batch_size=32, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e367fb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        weights = torchvision.models.ResNet101_Weights.IMAGENET1K_V1\n",
    "        self.resnet = resnet101(weights=weights)\n",
    "        for parameter in self.resnet.parameters():\n",
    "            parameter.requires_grad = False\n",
    "        self.preprocess = weights.transforms()\n",
    "        self.resnet.fc = nn.Sequential(nn.Linear(self.resnet.fc.in_features, 300))\n",
    "        self.lstm = nn.LSTM(input_size=300, hidden_size=256, num_layers=3)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, trainingData.n)\n",
    "\n",
    "    def forward(self, x_3d):\n",
    "        hidden = None\n",
    "        for t in range(x_3d.size(1)):\n",
    "            x = x_3d[:, t, :, :]\n",
    "            x = self.preprocess(x)\n",
    "            x = self.resnet(x)\n",
    "            out, hidden = self.lstm(x.unsqueeze(0), hidden)         \n",
    "        x = self.fc1(out[-1, :, :])\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "def accuracy(outputs, labels):\n",
    "    with torch.no_grad():\n",
    "        eq = outputs.argmax(1).to(\"cpu\") == labels\n",
    "        return (eq.sum() / eq.numel()).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8c09159",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(\"cuda\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe38c0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 299/299 [40:29<00:00,  8.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.615953241303613 0.0\n",
      "eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 119/119 [11:11<00:00,  5.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.612613173092113 0.0\n",
      "epoch 2\n",
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 299/299 [40:01<00:00,  8.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.611458781570894 0.0\n",
      "eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 119/119 [10:52<00:00,  5.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.60995415679547 0.0\n",
      "epoch 3\n",
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 299/299 [39:43<00:00,  7.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.603698441814818 0.0\n",
      "eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 119/119 [10:39<00:00,  5.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.613977267962544 0.0\n",
      "epoch 4\n",
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████████████████████████▌                                                      | 98/299 [12:59<27:39,  8.26s/it]"
     ]
    }
   ],
   "source": [
    "lossTrainHist = []\n",
    "lossValidationHist = []\n",
    "accuracyTrainHist = []\n",
    "accuracyValidationHist = []\n",
    "minLoss = float('inf')\n",
    "\n",
    "for epoch in range(300):\n",
    "    print(\"epoch\",epoch+1)\n",
    "    \n",
    "    runningTrainLoss = 0.0\n",
    "    runningValidationLoss = 0.0\n",
    "    \n",
    "    net.train()\n",
    "    print(\"train\")\n",
    "    for i, data in enumerate(tqdm(trainDataloader), 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs.to(\"cuda\"))\n",
    "        \n",
    "        lossTrain = criterion(outputs, labels.to(\"cuda\"))\n",
    "        lossTrain.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        runningTrainLoss += lossTrain.item()\n",
    "        accuracyTrainHist.append(accuracy(outputs, labels))\n",
    "        \n",
    "    lossTrainHist.append(runningTrainLoss/(i+1))\n",
    "    print(lossTrainHist[-1], accuracyTrainHist[-1])\n",
    "\n",
    "    net.eval()\n",
    "    print(\"eval\")\n",
    "    for i, data in enumerate(tqdm(validationDataloader), 0):\n",
    "        inputs, labels = data\n",
    "        with torch.no_grad():\n",
    "            outputs = net(inputs.to(\"cuda\"))\n",
    "\n",
    "        lossValidation = criterion(outputs, labels.to(\"cuda\"))\n",
    "        \n",
    "        runningValidationLoss += lossValidation.item()\n",
    "        accuracyValidationHist.append(accuracy(outputs, labels))\n",
    "        \n",
    "    lossValidationHist.append(runningValidationLoss/(i+1))\n",
    "    print(lossValidationHist[-1], accuracyValidationHist[-1])\n",
    "    \n",
    "    if minLoss>lossValidationHist[epoch]:\n",
    "        minLoss = lossValidationHist[epoch]\n",
    "        bestWeights1 = net.state_dict().copy()\n",
    "        epochSave = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1d0324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
